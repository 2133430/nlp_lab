{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_ #!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128   # dimension of word vector\n",
    "hidden_size = 1024 #feature\n",
    "num_layers =1\n",
    "num_epochs =5\n",
    "num_samples =1000\n",
    "batch_size =20\n",
    "seq_length =30\n",
    "learning_rate =0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx= {}\n",
    "        self.idx2word= {}\n",
    "        self.idx = 0;\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx+=1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary() #elem dictionary init as Dictionary object\n",
    "    \n",
    "    def get_data(self, file_path, batch_size=20):\n",
    "        #First: add words into the dictionary\n",
    "        with open(file_path, 'r') as file:\n",
    "            num_tokens =0 \n",
    "            # every line\n",
    "            for line in file:\n",
    "                words = line.split() +['<eos>'] #end of sentence\n",
    "                num_tokens += len(words)\n",
    "                #every word\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        \n",
    "        #Second: fenci tokenize\n",
    "        ids = torch.LongTensor(num_tokens) #64 bit interger size: num_tokens\n",
    "        num_token = 0\n",
    "        with open(file_path,'r') as file:\n",
    "            for line in file:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[num_token] = self.dictionary.word2idx[word]\n",
    "                    num_token +=1\n",
    "                    \n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches * batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the data\n",
    "#init as Corpus object\n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('data/train.txt',batch_size)\n",
    "vocab_size =len(corpus.dictionary)\n",
    "num_batches = ids.size(1) //seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 46479])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape # ids.view(batch_size,-1) # the size -1 is inferred from other dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  27,  930,   42,  ...,  392, 4864,   26])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.view(20,-1)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()#inheritage\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        #embedding\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        #(batch_size * seq_len, hidden_size)\n",
    "        out = out.reshape(out.size(0) * out.size(1) , out.size(2))\n",
    "        \n",
    "        out = self.linear(out)\n",
    "        return out, (h,c)\n",
    "\n",
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimize\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46449"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.size(1) - seq_length # from head to tail sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代到第[1/5]轮，step [0/1549], Loss9.2126, 困惑度: 10022.68\n",
      "迭代到第[1/5]轮，step [100/1549], Loss5.9987, 困惑度: 402.89\n",
      "迭代到第[1/5]轮，step [200/1549], Loss5.9280, 困惑度: 375.39\n",
      "迭代到第[1/5]轮，step [300/1549], Loss5.7989, 困惑度: 329.94\n",
      "迭代到第[1/5]轮，step [400/1549], Loss5.6805, 困惑度: 293.09\n",
      "迭代到第[1/5]轮，step [500/1549], Loss5.1068, 困惑度: 165.15\n",
      "迭代到第[1/5]轮，step [600/1549], Loss5.2065, 困惑度: 182.46\n",
      "迭代到第[1/5]轮，step [700/1549], Loss5.3672, 困惑度: 214.25\n",
      "迭代到第[1/5]轮，step [800/1549], Loss5.2160, 困惑度: 184.19\n",
      "迭代到第[1/5]轮，step [900/1549], Loss5.0603, 困惑度: 157.64\n",
      "迭代到第[1/5]轮，step [1000/1549], Loss5.1421, 困惑度: 171.07\n",
      "迭代到第[1/5]轮，step [1100/1549], Loss5.3135, 困惑度: 203.05\n",
      "迭代到第[1/5]轮，step [1200/1549], Loss5.1753, 困惑度: 176.85\n",
      "迭代到第[1/5]轮，step [1300/1549], Loss5.0651, 困惑度: 158.40\n",
      "迭代到第[1/5]轮，step [1400/1549], Loss4.8805, 困惑度: 131.69\n",
      "迭代到第[1/5]轮，step [1500/1549], Loss5.1535, 困惑度: 173.03\n",
      "迭代到第[2/5]轮，step [0/1549], Loss5.4517, 困惑度: 233.16\n",
      "迭代到第[2/5]轮，step [100/1549], Loss4.4988, 困惑度: 89.91\n",
      "迭代到第[2/5]轮，step [200/1549], Loss4.6177, 困惑度: 101.26\n",
      "迭代到第[2/5]轮，step [300/1549], Loss4.6637, 困惑度: 106.03\n",
      "迭代到第[2/5]轮，step [400/1549], Loss4.5813, 困惑度: 97.64\n",
      "迭代到第[2/5]轮，step [500/1549], Loss4.0843, 困惑度: 59.40\n",
      "迭代到第[2/5]轮，step [600/1549], Loss4.5289, 困惑度: 92.66\n",
      "迭代到第[2/5]轮，step [700/1549], Loss4.4575, 困惑度: 86.27\n",
      "迭代到第[2/5]轮，step [800/1549], Loss4.4368, 困惑度: 84.51\n",
      "迭代到第[2/5]轮，step [900/1549], Loss4.1861, 困惑度: 65.76\n",
      "迭代到第[2/5]轮，step [1000/1549], Loss4.3560, 困惑度: 77.94\n",
      "迭代到第[2/5]轮，step [1100/1549], Loss4.4636, 困惑度: 86.80\n",
      "迭代到第[2/5]轮，step [1200/1549], Loss4.4472, 困惑度: 85.38\n",
      "迭代到第[2/5]轮，step [1300/1549], Loss4.2205, 困惑度: 68.07\n",
      "迭代到第[2/5]轮，step [1400/1549], Loss4.0005, 困惑度: 54.62\n",
      "迭代到第[2/5]轮，step [1500/1549], Loss4.3499, 困惑度: 77.47\n",
      "迭代到第[3/5]轮，step [0/1549], Loss4.4797, 困惑度: 88.21\n",
      "迭代到第[3/5]轮，step [100/1549], Loss3.8418, 困惑度: 46.61\n",
      "迭代到第[3/5]轮，step [200/1549], Loss4.0125, 困惑度: 55.29\n",
      "迭代到第[3/5]轮，step [300/1549], Loss3.9081, 困惑度: 49.80\n",
      "迭代到第[3/5]轮，step [400/1549], Loss3.9091, 困惑度: 49.86\n",
      "迭代到第[3/5]轮，step [500/1549], Loss3.3465, 困惑度: 28.40\n",
      "迭代到第[3/5]轮，step [600/1549], Loss3.8670, 困惑度: 47.80\n",
      "迭代到第[3/5]轮，step [700/1549], Loss3.8205, 困惑度: 45.63\n",
      "迭代到第[3/5]轮，step [800/1549], Loss3.8036, 困惑度: 44.86\n",
      "迭代到第[3/5]轮，step [900/1549], Loss3.4772, 困惑度: 32.37\n",
      "迭代到第[3/5]轮，step [1000/1549], Loss3.6600, 困惑度: 38.86\n",
      "迭代到第[3/5]轮，step [1100/1549], Loss3.6864, 困惑度: 39.90\n",
      "迭代到第[3/5]轮，step [1200/1549], Loss3.6969, 困惑度: 40.32\n",
      "迭代到第[3/5]轮，step [1300/1549], Loss3.4769, 困惑度: 32.36\n",
      "迭代到第[3/5]轮，step [1400/1549], Loss3.2983, 困惑度: 27.07\n",
      "迭代到第[3/5]轮，step [1500/1549], Loss3.5945, 困惑度: 36.40\n",
      "迭代到第[4/5]轮，step [0/1549], Loss3.5621, 困惑度: 35.24\n",
      "迭代到第[4/5]轮，step [100/1549], Loss3.3069, 困惑度: 27.30\n",
      "迭代到第[4/5]轮，step [200/1549], Loss3.4559, 困惑度: 31.69\n",
      "迭代到第[4/5]轮，step [300/1549], Loss3.2821, 困惑度: 26.63\n",
      "迭代到第[4/5]轮，step [400/1549], Loss3.3749, 困惑度: 29.22\n"
     ]
    }
   ],
   "source": [
    "#backward\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "#train\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #init\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "             torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        #every mini-batch\n",
    "        inputs = ids[:, i: i+seq_length].to(device) #[ , one seq]\n",
    "        targets = ids[:,(i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        #forward\n",
    "        states = detach(states)\n",
    "        outputs,states  = model(inputs, states)\n",
    "        loss = criterion(outputs,targets.reshape(-1)) # (h - y) yi chuan!!! sum\n",
    "        \n",
    "        #optimize\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(),0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        step = (i+1)// seq_length\n",
    "        if step %100 ==0:\n",
    "            print('迭代到第[{}/{}]轮，step [{}/{}], Loss{:.4f}, 困惑度: {:5.2f}'\n",
    "                  .format(epoch+1,num_epochs,step,num_batches, loss.item(), np.exp(loss.item())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open('sample.txt', 'w') as f:\n",
    "\n",
    "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "\n",
    "        #random input\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # forward\n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # predic\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # result as next input\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            #out\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('生成了 [{}/{}] 个词，存储到 {}'.format(i+1, num_samples, 'sample.txt'))\n",
    "\n",
    "# checkpoints\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
