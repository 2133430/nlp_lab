{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "import jieba\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. input train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input the csv file into the pandas dataframe with specific category'cat,news_content'(10000,2)\n",
    "#sep = seperator\n",
    "#pd.dropna drop the data missing some elem\n",
    "news_data = pd.read_csv(\"./data/cnews.train.txt\",sep = \"\\t\",names=['category','news_content'],encoding='utf-8')\n",
    "news_data = news_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport = news_data[news_data.category=='体育']\n",
    "#split the 'news_content' and get the value part (nparray)and put it into the list\n",
    "#in this way, we can get the input data and get the category as output target\n",
    "sport = sport.news_content.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = news_data[news_data.category=='家居']#select category\n",
    "home = home.news_content.values.tolist()#select value\n",
    "fashion = news_data[news_data.category=='时尚']\n",
    "fashion = fashion.news_content.values.tolist()\n",
    "house = news_data[news_data.category=='房产']\n",
    "house = house.news_content.values.tolist()\n",
    "edu = news_data[news_data.category=='教育']\n",
    "edu = edu.news_content.values.tolist()\n",
    "politic = news_data[news_data.category=='时政']\n",
    "politic = politic.news_content.values.tolist()\n",
    "enter = news_data[news_data.category=='娱乐']\n",
    "enter = enter.news_content.values.tolist()\n",
    "game = news_data[news_data.category=='游戏']\n",
    "game = game.news_content.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'仿古地板流行更适合大住宅冬的木地板似乎也受到了“古风”熏染，通体上下透出一股浓浓的“复古味”。据了解，这些复古木地板大多采用手工制作，刻意对木纹进行强化与突出处理，让木地板变得更加生动立体，更富于怀旧气息。也就是目前流行市场的仿古地板。回归自然，个性消费和资源的有效利用是仿古流行的主要因素，让家和自然更接近是现代装修的主题，更具个性的仿古手工制造符合了人无我有的消费个性风格，这是仿古地板风行未来地板市场的理由。有两种客户最有可能成为仿古地板的消费者，一种是欣赏仿古风格，让家更有个性的消费者，这是对地板材种有精确理解，追求厚实、粗犷，原始居家风格的行家，仿古地板一开始好像就是为他们发明的。第二种是时尚消费的盲从者，对仿古地板知其然而不知其所以然，因为时尚，所以购买。选购时要掌握三个要点：1.仿古地板一般不是珍稀名贵地板，材料成本可能比同品种地板略低，因为是手工制造，贵在工艺制造成本；2.想清楚了再买，要分清自己是仿古地板两种消费类型中的哪一种，金象恒森地板更喜欢深识仿古地板的行家购买；3.要注意和家居装修风格的一体性，仿古地板更适合大平方米的住宅，家居面积太小不宜用仿古地板。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word\n",
    "\n",
    "stopwords = pd.read_csv(\"./data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'],encoding='utf-8')\n",
    "stopwords = stopwords['stopword'].values # put it into the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '\"', '#', ..., '07', '08', '09'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.955 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#corpus\n",
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            sens = jieba.lcut(line)#cut the everyline into the word\n",
    "            sens = filter(lambda x:len(x)>1,sens)#frequncy\n",
    "            sens = filter(lambda x:x not in stopwords,sens)#stop\n",
    "            sentences.append((\" \".join(sens),category))#!!!!!\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            continue\n",
    "\n",
    "sentences = []\n",
    "\n",
    "preprocess_text(sport, sentences, 'sport')\n",
    "preprocess_text(home,sentences, 'home')\n",
    "preprocess_text(fashion,sentences, 'fashion')\n",
    "preprocess_text(house, sentences, 'house')\n",
    "preprocess_text(edu, sentences, 'edu')\n",
    "preprocess_text(politic, sentences, 'politic')\n",
    "preprocess_text(enter, sentences, 'entertainment')\n",
    "preprocess_text(game,sentences, 'game')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#former dictionary built\n",
    "#split the data set into different sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)\n",
    "train_data, test_data, train_target, test_target = train_test_split(x, y, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "learn = tf.contrib.learn\n",
    "\n",
    "FLAGS =None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "MIN_WORD_FREQUENCE = 2\n",
    "\n",
    "EMBEDDING_SIZE = 20\n",
    "#CORE???\n",
    "N_FILTERS =10\n",
    "WINDOW_SIZE =20\n",
    "FILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE] #20*20\n",
    "FILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS] #20*10\n",
    "\n",
    "#POOLING\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "n_words = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LM\n",
    "def cnn_model(feature, target):\n",
    "    # word_feature_weight_matrix n_words * embedding_size\n",
    "    #!!!!!!!!input2matrix[batch_size,seq_len,embedding_size]\n",
    "    target =tf.one_hot(target, 15, 1, 0)\n",
    "    #!!!!!!!Embedding\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(feature, vocab_size =n_words, embed_dim = EMBEDDING_SIZE, scope ='words')\n",
    "    word_vectors = tf.expand_dims(word_vectors,3)\n",
    "    \n",
    "    #every layer has it own convolution and pooling\n",
    "    #First Layer\n",
    "    with tf.variable_scope('CNN_Layer1'):\n",
    "        conv1 = tf.contrib.layers.convolution2d(\n",
    "                word_vectors, N_FILTERS, FILTER_SHAPE1, padding = 'VALID')\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        #maxpool\n",
    "        pool1 =tf.nn.max_pool(\n",
    "                conv1,\n",
    "                ksize= [1,POOLING_WINDOW,1 ,1],\n",
    "                strides =[1, POOLING_STRIDE,1 ,1],\n",
    "                padding = 'SAME')\n",
    "        #x.t\n",
    "        pool1 = tf.transpose(pool1,[0,1,3,2])\n",
    "    #Second Layer\n",
    "    with tf.variable_scope('CNN_Layer2'):\n",
    "        conv2 = tf.contrib.layers.convolution2d(pool1, N_FILTERS, FILTER_SHAPE2, padding= 'VALID')\n",
    "        \n",
    "        pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])\n",
    "    \n",
    "    #The third fully connected\n",
    "    logits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn=None)#!!!!!!\n",
    "    #the merely connect layer \n",
    "    loss =tf.losses.softmax_cross_entropy(target, logits)\n",
    "    \n",
    "    train_op =tf.contrib.layers.optimize_loss(\n",
    "                loss,\n",
    "                tf.contrib.framework.get_global_step(),\n",
    "                optimizer ='Adam',\n",
    "                learning_rate=0.01)\n",
    "    return({\n",
    "        'class': tf.argmax(logits,1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    },loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-101cbea574b0>:3: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Total words: 48058\n"
     ]
    }
   ],
   "source": [
    "global n_words\n",
    "# 处理词汇\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=MIN_WORD_FREQUENCE)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data)))\n",
    "x_test = np.array(list(vocab_processor.transform(test_data)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list by hand\n",
    "cate_dic = {'sport':1, 'home':2, 'fashion':3, 'house':4, 'edu':5, 'politic':6, 'entertainment':7,'game':8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yingshe\n",
    "train_target = map(lambda x:cate_dic[x], train_target)\n",
    "test_target = map(lambda x:cate_dic[x], test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pandas.Series(train_target)\n",
    "y_test = pandas.Series(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp9rj211md\n",
      "INFO:tensorflow:Using config: {'_master': '', '_eval_distribute': None, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_evaluation_master': '', '_is_chief': True, '_session_config': None, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_environment': 'local', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3e5a223320>, '_device_fn': None, '_num_worker_replicas': 0, '_model_dir': '/tmp/tmp9rj211md', '_protocol': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_task_id': 0, '_task_type': None, '_train_distribute': None, '_num_ps_replicas': 0}\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1241: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp9rj211md/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.708875, step = 0\n",
      "INFO:tensorflow:global_step/sec: 96.6349\n",
      "INFO:tensorflow:loss = 0.43337888, step = 100 (1.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.461\n",
      "INFO:tensorflow:loss = 0.12586406, step = 200 (0.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.665\n",
      "INFO:tensorflow:loss = 0.004664298, step = 300 (0.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.801\n",
      "INFO:tensorflow:loss = 0.0047971043, step = 400 (0.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.018\n",
      "INFO:tensorflow:loss = 0.0066941325, step = 500 (0.846 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.053\n",
      "INFO:tensorflow:loss = 0.019181626, step = 600 (0.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.403\n",
      "INFO:tensorflow:loss = 0.12315087, step = 700 (0.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.66\n",
      "INFO:tensorflow:loss = 0.00069616735, step = 800 (0.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.199\n",
      "INFO:tensorflow:loss = 0.008272992, step = 900 (0.853 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp9rj211md/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.003472688.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp9rj211md/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy: 0.893500\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=cnn_model))\n",
    "\n",
    "# 训练和预测\n",
    "classifier.fit(x_train, y_train, steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "def rnn_model(features, target):\n",
    "    \n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "            features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(target, 15, 1, 0)\n",
    "    logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss,\n",
    "            tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adam',\n",
    "            learning_rate=0.01)\n",
    "\n",
    "    return ({\n",
    "            'class': tf.argmax(logits, 1),\n",
    "            'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_mt7gefp\n",
      "INFO:tensorflow:Using config: {'_master': '', '_eval_distribute': None, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_evaluation_master': '', '_is_chief': True, '_session_config': None, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_environment': 'local', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3e597fd588>, '_device_fn': None, '_num_worker_replicas': 0, '_model_dir': '/tmp/tmp_mt7gefp', '_protocol': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_task_id': 0, '_task_type': None, '_train_distribute': None, '_num_ps_replicas': 0}\n",
      "WARNING:tensorflow:From <ipython-input-31-b8250f403472>:27: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:399: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:147: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp_mt7gefp/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.7080333, step = 0\n",
      "INFO:tensorflow:global_step/sec: 7.63164\n",
      "INFO:tensorflow:loss = 0.06986313, step = 100 (13.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.1892\n",
      "INFO:tensorflow:loss = 0.022343682, step = 200 (5.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.2091\n",
      "INFO:tensorflow:loss = 0.0060605872, step = 300 (5.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.984\n",
      "INFO:tensorflow:loss = 0.0026015341, step = 400 (5.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0888\n",
      "INFO:tensorflow:loss = 0.001477683, step = 500 (5.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.2448\n",
      "INFO:tensorflow:loss = 0.0011096412, step = 600 (5.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0265\n",
      "INFO:tensorflow:loss = 0.0008229724, step = 700 (5.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.2166\n",
      "INFO:tensorflow:loss = 0.0007243293, step = 800 (5.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0909\n",
      "INFO:tensorflow:loss = 0.00060244836, step = 900 (5.527 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp_mt7gefp/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0004471108.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp_mt7gefp/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy: 0.956000\n"
     ]
    }
   ],
   "source": [
    "model_fn = rnn_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
